config_name: ludovico_mini
device: cuda

# default_config
model_type: gpt
gpt: {n_layer: None, n_head: None, n_embd: None}

# GPT-1
openai_gpt: {n_layer: 12, n_head: 12, n_embd: 768}  # 117M params
# GPT-2 configs
gpt2: {n_layer: 12, n_head: 12, n_embd: 768}  # 124M params
gpt2_medium: {n_layer: 24, n_head: 16, n_embd: 1024} # 350M params
gpt2_large: {n_layer: 36, n_head: 20, n_embd: 1280} # 774M params
gpt2_xl: {n_layer: 48, n_head: 25, n_embd: 1600} # 1558M params
# Gophers
gopher_44m: {n_layer: 8, n_head: 16, n_embd: 512}
# Custom 
gpt_mini: {n_layer: 12, n_head: 12, n_embd: 192}
gpt_micro: {n_layer: 4, n_head: 4, n_embd: 128}
gpt_nano: {n_layer: 3, n_head: 3, n_embd: 48}

gpt_bach_old: {n_layer: 4, n_head: 8, n_embd: 128}
gpt_bach: {n_layer: 8, n_head: 16, n_embd: 256}

vocab_size: None
block_size: None

embd_pdrop: 0.1
resid_pdrop: 0.1
attn_pdrop: 0.1

ludovico_mini_old:
  device: cuda
  num_hiddens: 128
  num_residual_layers: 2
  num_residual_hiddens: 32
  num_embeddings: 16
  embedding_dim: 128
  commitment_cost: 0.25
  decay: 0.99

ludovico_mini:
  device: cuda
  num_hiddens: 256
  num_residual_layers: 4
  num_residual_hiddens: 64
  num_embeddings: 32
  embedding_dim: 256
  commitment_cost: 0.25
  decay: 0.99

trainer:
  device: cuda
  # dataloder parameters
  num_workers: 8
  # optimizer parameters
  max_iters: None
  multiplier: 500
  batch_size: 128
  learning_rate: 0.001
  betas: {lower: 0.9, upper: 0.95}
  weight_decay: 0.1 # only applied on matmul weights
  grad_norm_clip: 1.0

va_vae_trainer:
  device: cuda
  batch_size: 128
  learning_rate: 0.001
  epochs: 20
